{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    # 加载数据,\n",
    "    def __init__(self, path=\"./Data/\", dataset=\"frappe\", loss_type=\"square_loss\"):\n",
    "        self.dataset = dataset\n",
    "        self.loss_type = loss_type\n",
    "        self.path = path + dataset + \"/\"\n",
    "        self.trainfile = self.path + dataset + \".train.libfm\"\n",
    "        self.testfile = self.path + dataset + \".test.libfm\"\n",
    "        self.validationfile = self.path + dataset + \".validation.libfm\"\n",
    "        self.features_M = {}\n",
    "        self.construct_df()\n",
    "\n",
    "    #         self.Train_data, self.Validation_data, self.Test_data = self.construct_data( loss_type )\n",
    "\n",
    "    def construct_df(self):\n",
    "        self.data_train = pd.read_table(self.trainfile, sep=\" \", header=None, engine='python')\n",
    "        self.data_test = pd.read_table(self.testfile, sep=\" \", header=None, engine=\"python\")\n",
    "        self.data_valid = pd.read_table(self.validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "        #       第一列是标签，y\n",
    "\n",
    "        # for i in self.data_test.columns[1:]:\n",
    "        #     self.data_test[i] = self.data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "        #     self.data_train[i] = self.data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "        #     self.data_valid[i] = self.data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "        # self.all_data = pd.concat([self.data_train, self.data_test, self.data_valid])\n",
    "        \n",
    "        # self.field_dims = []\n",
    "        # \n",
    "        # for i in self.all_data.columns[1:]:\n",
    "        # \n",
    "        #     maps = {val: k for k, val in enumerate(set(self.all_data[i]))}\n",
    "        #     self.data_test[i] = self.data_test[i].map(maps)\n",
    "        #     self.data_train[i] = self.data_train[i].map(maps)\n",
    "        #     self.data_valid[i] = self.data_valid[i].map(maps)\n",
    "        #     self.features_M[i] = maps\n",
    "        #     self.field_dims.append(len(set(self.all_data[i])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "trainfile = \"frappe.train.libfm\"\n",
    "testfile = \"frappe.test.libfm\"\n",
    "validationfile = \"frappe.validation.libfm\"     \n",
    "data_train = pd.read_table(trainfile, sep=\" \", header=None, engine='python')\n",
    "data_test = pd.read_table(testfile, sep=\" \", header=None, engine=\"python\")\n",
    "data_valid = pd.read_table(validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "#       第一列是标签，y\n",
    "\n",
    "for i in data_test.columns[1:]:\n",
    "    data_test[i] = data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    data_train[i] = data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    data_valid[i] = data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "all_data = pd.concat([data_train, data_test, data_valid])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                  0              1              2              3   \\\ncount  288571.000000  288571.000000  288571.000000  288571.000000   \nmean       -0.333315     189.647782    2457.049548    5041.289991   \nstd         0.942817     156.459127    1300.981561       1.782010   \nmin        -1.000000       0.000000     957.000000    5039.000000   \n25%        -1.000000      74.000000    1185.000000    5040.000000   \n50%        -1.000000     153.000000    2204.000000    5041.000000   \n75%         1.000000     271.000000    3577.000000    5043.000000   \nmax         1.000000     956.000000    5038.000000    5045.000000   \n\n                  4              5              6              7   \\\ncount  288571.000000  288571.000000  288571.000000  288571.000000   \nmean     5048.999439    5053.659796    5055.262944    5058.050864   \nstd         2.037305       0.473778       0.541126       0.219721   \nmin      5046.000000    5053.000000    5055.000000    5058.000000   \n25%      5047.000000    5053.000000    5055.000000    5058.000000   \n50%      5049.000000    5054.000000    5055.000000    5058.000000   \n75%      5051.000000    5054.000000    5055.000000    5058.000000   \nmax      5052.000000    5054.000000    5057.000000    5059.000000   \n\n                  8              9              10  \ncount  288571.000000  288571.000000  288571.000000  \nmean     5061.402702    5077.835950    5171.361072  \nstd         1.385556      12.110474      35.535266  \nmin      5060.000000    5069.000000    5149.000000  \n25%      5060.000000    5070.000000    5149.000000  \n50%      5061.000000    5074.000000    5153.000000  \n75%      5062.000000    5080.000000    5180.000000  \nmax      5068.000000    5148.000000    5381.000000  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(all_data.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "a=all_data.iloc[0].values[1:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "a = np.array(a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "class LoadData():\n",
    "    # 加载数据,\n",
    "    def __init__(self, path=\"./Data/\", dataset=\"frappe\", loss_type=\"square_loss\"):\n",
    "        self.dataset = dataset\n",
    "        # self.loss_type = loss_type\n",
    "        self.path = path + dataset + \"/\"\n",
    "        self.trainfile = self.path + dataset + \".train.libfm\"\n",
    "        self.testfile = self.path + dataset + \".test.libfm\"\n",
    "        self.validationfile = self.path + dataset + \".validation.libfm\"\n",
    "        # self.features_M = {}\n",
    "        self.construct_df()\n",
    "\n",
    "    #         self.Train_data, self.Validation_data, self.Test_data = self.construct_data( loss_type )\n",
    "\n",
    "    def construct_df(self):\n",
    "        self.data_train = pd.read_table(self.trainfile, sep=\" \", header=None, engine='python')\n",
    "        self.data_test = pd.read_table(self.testfile, sep=\" \", header=None, engine=\"python\")\n",
    "        self.data_valid = pd.read_table(self.validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "        #       第一列是标签，y\n",
    "\n",
    "        for i in self.data_test.columns[1:]:\n",
    "            self.data_test[i] = self.data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "            self.data_train[i] = self.data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "            self.data_valid[i] = self.data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "        # self.all_data = pd.concat([self.data_train, self.data_test, self.data_valid])\n",
    "        # self.field_dims = []\n",
    "\n",
    "        # for i in self.all_data.columns[1:]:\n",
    "        #\n",
    "        #     maps = {val: k for k, val in enumerate(set(self.all_data[i]))}\n",
    "        #     self.data_test[i] = self.data_test[i].map(maps)\n",
    "        #     self.data_train[i] = self.data_train[i].map(maps)\n",
    "        #     self.data_valid[i] = self.data_valid[i].map(maps)\n",
    "        #     self.features_M[i] = maps\n",
    "        #     self.field_dims.append(len(set(self.all_data[i])))\n",
    "\n",
    "\n",
    "class RecData():\n",
    "    # define the dataset\n",
    "    def __init__(self, all_data):\n",
    "        self.data_df = all_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_df.iloc[idx].values[1:]\n",
    "        y1 = self.data_df.iloc[idx].values[0]\n",
    "        return x, y1\n",
    "\n",
    "def getdataloader_frappe(path=\"../.././data/data2/\",dataset=\"frappe\",num_ng=4,batch_size=256):\n",
    "    # 加载数据\n",
    "    print(\"load frappe data\")\n",
    "    DataF = LoadData(path=path, dataset=dataset)\n",
    "    # 直接保存\n",
    "    datatrain = RecData(DataF.data_train)\n",
    "    datavalid = RecData(DataF.data_valid)\n",
    "    datatest = RecData(DataF.data_test)\n",
    "    print(\"datatest\",len(datatest))\n",
    "    print(\"datatrain\",len(datatrain))\n",
    "    print(\"datavalid\",len(datavalid))\n",
    "    trainLoader = torch.utils.data.DataLoader(datatrain, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True,drop_last=True)\n",
    "    validLoader = torch.utils.data.DataLoader(datavalid, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True)\n",
    "    testLoader = torch.utils.data.DataLoader(datatest, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True)\n",
    "    return trainLoader, validLoader, testLoader,datatrain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "load frappe data\n",
      "datatest 28842\ndatatrain 230887\ndatavalid 28842\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trainLoader, validLoader, testLoader, datatrain =  getdataloader_frappe(path=\"../\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "iters = iter(testLoader)\n",
    "a  = iters.next()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "x,y = datatrain[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 451, 4149, 5041, 5046, 5053, 5055, 5058, 5060, 5069, 5149])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def my_mse_loss(pred,label):\n",
    "    \n",
    "    return np.sum((np.array(pred)-np.array(label))**2)/len(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.6123724356957945\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_true = np.minimum(y_true,np.zeros(len(y_true))*1.0)\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(mean_squared_error(y_pred,y_true)**0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.6123724356957945\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "func = my_mse_loss\n",
    "print(func(y_true,y_pred)**0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}