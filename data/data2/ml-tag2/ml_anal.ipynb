{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    # 加载数据,\n",
    "    def __init__(self, path=\"./Data/\", dataset=\"frappe\", loss_type=\"square_loss\"):\n",
    "        self.dataset = dataset\n",
    "        self.loss_type = loss_type\n",
    "        self.path = path + dataset + \"/\"\n",
    "        self.trainfile = self.path + dataset + \".train.libfm\"\n",
    "        self.testfile = self.path + dataset + \".test.libfm\"\n",
    "        self.validationfile = self.path + dataset + \".validation.libfm\"\n",
    "        self.features_M = {}\n",
    "        self.construct_df()\n",
    "\n",
    "    #         self.Train_data, self.Validation_data, self.Test_data = self.construct_data( loss_type )\n",
    "\n",
    "    def construct_df(self):\n",
    "        self.data_train = pd.read_table(self.trainfile, sep=\" \", header=None, engine='python')\n",
    "        self.data_test = pd.read_table(self.testfile, sep=\" \", header=None, engine=\"python\")\n",
    "        self.data_valid = pd.read_table(self.validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "        #       第一列是标签，y\n",
    "\n",
    "        for i in self.data_test.columns[1:]:\n",
    "            self.data_test[i] = self.data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "            self.data_train[i] = self.data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "            self.data_valid[i] = self.data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "        # self.all_data = pd.concat([self.data_train, self.data_test, self.data_valid])\n",
    "        \n",
    "        self.field_dims = []\n",
    "\n",
    "        for i in self.all_data.columns[1:]:\n",
    "\n",
    "            maps = {val: k for k, val in enumerate(set(self.all_data[i]))}\n",
    "            self.data_test[i] = self.data_test[i].map(maps)\n",
    "            self.data_train[i] = self.data_train[i].map(maps)\n",
    "            self.data_valid[i] = self.data_valid[i].map(maps)\n",
    "            self.features_M[i] = maps\n",
    "            self.field_dims.append(len(set(self.all_data[i])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "features_M = {}\n",
    "trainfile = \"ml-tag.train.libfm\"\n",
    "testfile = \"ml-tag.test.libfm\"\n",
    "validationfile = \"ml-tag.validation.libfm\"     \n",
    "data_train = pd.read_table(trainfile, sep=\" \", header=None, engine='python')\n",
    "data_test = pd.read_table(testfile, sep=\" \", header=None, engine=\"python\")\n",
    "data_valid = pd.read_table(validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "#       第一列是标签，y\n",
    "\n",
    "for i in data_test.columns[1:]:\n",
    "    data_test[i] = data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    data_train[i] = data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    data_valid[i] = data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "all_data = pd.concat([data_train, data_test, data_valid])\n",
    "field_dims = []\n",
    "\n",
    "for i in all_data.columns[1:]:\n",
    "    # if self.dataset != \"frappe\":\n",
    "        # maps = {}\n",
    "    maps = {val: k+offsets[i-1] for k, val in enumerate(set(all_data[i]))}\n",
    "    data_test[i] = data_test[i].map(maps)\n",
    "    data_train[i] = data_train[i].map(maps)\n",
    "    data_valid[i] = data_valid[i].map(maps)\n",
    "    features_M[i] = maps\n",
    "    field_dims.append(len(set(all_data[i])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "                  0             1             2             3\ncount  1.605487e+06  1.605487e+06  1.605487e+06  1.605487e+06\nmean  -3.338744e-01  8.629221e+03  2.043862e+04  6.083390e+04\nstd    9.426179e-01  5.029680e+03  4.368120e+03  1.505964e+04\nmin   -1.000000e+00  0.000000e+00  1.704500e+04  4.078800e+04\n25%   -1.000000e+00  4.098000e+03  1.764900e+04  4.703700e+04\n50%   -1.000000e+00  9.157000e+03  1.881800e+04  5.861900e+04\n75%    1.000000e+00  1.284000e+04  2.118300e+04  7.364000e+04\nmax    1.000000e+00  1.704400e+04  4.078700e+04  9.044400e+04",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.605487e+06</td>\n      <td>1.605487e+06</td>\n      <td>1.605487e+06</td>\n      <td>1.605487e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-3.338744e-01</td>\n      <td>8.629221e+03</td>\n      <td>2.043862e+04</td>\n      <td>6.083390e+04</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.426179e-01</td>\n      <td>5.029680e+03</td>\n      <td>4.368120e+03</td>\n      <td>1.505964e+04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.704500e+04</td>\n      <td>4.078800e+04</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.000000e+00</td>\n      <td>4.098000e+03</td>\n      <td>1.764900e+04</td>\n      <td>4.703700e+04</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.000000e+00</td>\n      <td>9.157000e+03</td>\n      <td>1.881800e+04</td>\n      <td>5.861900e+04</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000e+00</td>\n      <td>1.284000e+04</td>\n      <td>2.118300e+04</td>\n      <td>7.364000e+04</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000e+00</td>\n      <td>1.704400e+04</td>\n      <td>4.078700e+04</td>\n      <td>9.044400e+04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "data_train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[    0 17045 40788]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "offsets = np.array([0,*np.cumsum(field_dims)[:-1]])\n",
    "print(offsets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "data_train.to_csv(\"ml-tag2.train.libfm\", sep=\" \",header=None,index=None)\n",
    "data_valid.to_csv(\"ml-tag2.valid.libfm\",sep=\" \",header=None,index=None)\n",
    "data_test.to_csv(\"ml-tag2.test.libfm\",sep=\" \",header=None,index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "class LoadData():\n",
    "    # 加载数据,\n",
    "    def __init__(self, path=\"./Data/\", dataset=\"frappe\", loss_type=\"square_loss\"):\n",
    "        self.dataset = dataset\n",
    "        # self.loss_type = loss_type\n",
    "        self.path = path + dataset + \"/\"\n",
    "        self.trainfile = self.path + dataset + \".train.libfm\"\n",
    "        self.testfile = self.path + dataset + \".test.libfm\"\n",
    "        self.validationfile = self.path + dataset + \".validation.libfm\"\n",
    "        # self.features_M = {}\n",
    "        self.construct_df()\n",
    "\n",
    "    #         self.Train_data, self.Validation_data, self.Test_data = self.construct_data( loss_type )\n",
    "\n",
    "    def construct_df(self):\n",
    "        self.data_train = pd.read_table(self.trainfile, sep=\" \", header=None, engine='python')\n",
    "        self.data_test = pd.read_table(self.testfile, sep=\" \", header=None, engine=\"python\")\n",
    "        self.data_valid = pd.read_table(self.validationfile, sep=\" \", header=None, engine=\"python\")\n",
    "        #       第一列是标签，y\n",
    "\n",
    "        # for i in self.data_test.columns[1:]:\n",
    "        #     self.data_test[i] = self.data_test[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "        #     self.data_train[i] = self.data_train[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "        #     self.data_valid[i] = self.data_valid[i].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "        # self.all_data = pd.concat([self.data_train, self.data_test, self.data_valid])\n",
    "        # self.field_dims = []\n",
    "\n",
    "        # for i in self.all_data.columns[1:]:\n",
    "        #\n",
    "        #     maps = {val: k for k, val in enumerate(set(self.all_data[i]))}\n",
    "        #     self.data_test[i] = self.data_test[i].map(maps)\n",
    "        #     self.data_train[i] = self.data_train[i].map(maps)\n",
    "        #     self.data_valid[i] = self.data_valid[i].map(maps)\n",
    "        #     self.features_M[i] = maps\n",
    "        #     self.field_dims.append(len(set(self.all_data[i])))\n",
    "\n",
    "\n",
    "class RecData():\n",
    "    # define the dataset\n",
    "    def __init__(self, all_data):\n",
    "        self.data_df = all_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_df.iloc[idx].values[1:]\n",
    "        y1 = self.data_df.iloc[idx].values[0]\n",
    "        return x, y1\n",
    "\n",
    "def getdataloader_frappe(path=\"../.././data/data2/\",dataset=\"ml-tag2\",num_ng=4,batch_size=256):\n",
    "    # 加载数据\n",
    "    print(\"load frappe data\")\n",
    "    DataF = LoadData(path=path, dataset=dataset)\n",
    "    # 直接保存\n",
    "    datatrain = RecData(DataF.data_train)\n",
    "    datavalid = RecData(DataF.data_valid)\n",
    "    datatest = RecData(DataF.data_test)\n",
    "    print(\"datatest\",len(datatest))\n",
    "    print(\"datatrain\",len(datatrain))\n",
    "    print(\"datavalid\",len(datavalid))\n",
    "    trainLoader = torch.utils.data.DataLoader(datatrain, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True,drop_last=True)\n",
    "    validLoader = torch.utils.data.DataLoader(datavalid, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True)\n",
    "    testLoader = torch.utils.data.DataLoader(datatest, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True)\n",
    "    return trainLoader, validLoader, testLoader,datatrain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "load frappe data\n",
      "datatest 200659\ndatatrain 1605487\ndatavalid 200640\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trainLoader, validLoader, testLoader, datatrain =  getdataloader_frappe(path=\"../\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "iters = iter(testLoader)\n",
    "a  = iters.next()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "x,y = datatrain[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[1.6757e+04, 1.7869e+04, 4.1303e+04],\n         [8.9790e+03, 1.9052e+04, 8.4916e+04],\n         [1.0368e+04, 3.9733e+04, 6.3146e+04],\n         [5.1940e+03, 1.7056e+04, 7.3874e+04],\n         [9.9290e+03, 1.8724e+04, 4.3090e+04],\n         [1.6651e+04, 1.7522e+04, 4.3289e+04],\n         [1.0622e+04, 2.0697e+04, 4.8156e+04],\n         [1.3156e+04, 1.9183e+04, 4.8518e+04],\n         [1.4520e+04, 1.9058e+04, 5.7988e+04],\n         [1.0622e+04, 3.1664e+04, 4.6053e+04],\n         [1.0622e+04, 1.8242e+04, 5.3608e+04],\n         [1.1700e+03, 2.9620e+04, 8.5096e+04],\n         [8.8500e+02, 1.7612e+04, 4.9388e+04],\n         [9.1570e+03, 3.7559e+04, 4.7192e+04],\n         [1.3563e+04, 2.5656e+04, 6.3870e+04],\n         [3.6630e+03, 1.7852e+04, 4.2873e+04],\n         [7.2590e+03, 3.1863e+04, 8.4022e+04],\n         [5.7980e+03, 1.7842e+04, 5.4159e+04],\n         [1.1050e+03, 1.7613e+04, 8.0229e+04],\n         [1.8980e+03, 1.8784e+04, 5.5719e+04],\n         [1.2770e+04, 2.0375e+04, 6.0668e+04],\n         [1.5089e+04, 2.5369e+04, 5.6829e+04],\n         [9.7100e+03, 1.8232e+04, 5.4704e+04],\n         [7.2780e+03, 1.8256e+04, 4.4026e+04],\n         [3.2020e+03, 3.2514e+04, 6.5402e+04],\n         [1.1157e+04, 1.9046e+04, 5.2423e+04],\n         [7.5750e+03, 2.0866e+04, 6.7168e+04],\n         [1.8980e+03, 1.8598e+04, 4.2731e+04],\n         [1.0728e+04, 1.9854e+04, 5.9214e+04],\n         [8.5540e+03, 1.8732e+04, 8.2253e+04],\n         [8.0140e+03, 2.1485e+04, 5.0590e+04],\n         [9.1080e+03, 1.8164e+04, 4.6668e+04],\n         [5.6750e+03, 1.7059e+04, 6.0015e+04],\n         [2.1840e+03, 1.7851e+04, 4.1995e+04],\n         [1.2959e+04, 2.6277e+04, 6.8561e+04],\n         [1.3425e+04, 2.1082e+04, 5.7602e+04],\n         [1.4206e+04, 3.0169e+04, 8.5599e+04],\n         [1.4520e+04, 2.4882e+04, 6.7708e+04],\n         [1.8980e+03, 3.0545e+04, 4.1289e+04],\n         [2.2620e+03, 1.7523e+04, 5.1603e+04],\n         [4.3990e+03, 1.8686e+04, 4.1599e+04],\n         [7.4000e+02, 2.0866e+04, 8.6686e+04],\n         [1.3464e+04, 2.4153e+04, 4.8993e+04],\n         [3.2540e+03, 1.8133e+04, 4.7300e+04],\n         [1.1320e+04, 1.8145e+04, 8.7922e+04],\n         [1.0154e+04, 1.9034e+04, 5.7398e+04],\n         [1.8030e+03, 1.7335e+04, 5.9269e+04],\n         [1.3889e+04, 1.7109e+04, 4.2546e+04],\n         [1.1460e+04, 1.7777e+04, 4.1341e+04],\n         [1.5209e+04, 1.7612e+04, 4.4473e+04],\n         [1.6404e+04, 1.7643e+04, 6.1162e+04],\n         [1.8980e+03, 2.8554e+04, 4.3535e+04],\n         [5.8280e+03, 2.0964e+04, 8.2306e+04],\n         [3.1390e+03, 1.7404e+04, 5.0223e+04],\n         [9.1570e+03, 2.5836e+04, 4.6676e+04],\n         [1.6981e+04, 2.0874e+04, 7.3160e+04],\n         [1.1540e+04, 1.7285e+04, 4.7794e+04],\n         [3.2890e+03, 1.8542e+04, 8.0762e+04],\n         [9.0400e+02, 1.7197e+04, 8.9768e+04],\n         [1.6250e+03, 1.8738e+04, 6.8054e+04],\n         [4.6290e+03, 1.8919e+04, 7.9261e+04],\n         [8.7800e+03, 1.7600e+04, 4.7134e+04],\n         [1.1663e+04, 1.9275e+04, 4.3864e+04],\n         [6.0300e+03, 1.7226e+04, 8.6723e+04],\n         [1.0411e+04, 1.7237e+04, 5.5225e+04],\n         [2.0440e+03, 1.8515e+04, 5.2444e+04],\n         [7.0520e+03, 1.7197e+04, 7.7226e+04],\n         [9.2040e+03, 1.8012e+04, 5.5372e+04],\n         [2.6910e+03, 3.4213e+04, 4.8255e+04],\n         [1.4665e+04, 1.7183e+04, 6.0254e+04],\n         [5.0090e+03, 1.7762e+04, 4.9915e+04],\n         [7.2750e+03, 2.0423e+04, 4.1107e+04],\n         [1.4520e+04, 2.4614e+04, 5.9655e+04],\n         [1.6651e+04, 1.9195e+04, 4.6850e+04],\n         [8.5780e+03, 1.7229e+04, 5.3185e+04],\n         [4.4690e+03, 2.0517e+04, 5.9774e+04],\n         [5.1940e+03, 2.0418e+04, 5.8878e+04],\n         [1.8980e+03, 1.9501e+04, 8.3871e+04],\n         [3.2990e+03, 1.7356e+04, 7.5783e+04],\n         [1.5353e+04, 1.7736e+04, 5.1034e+04],\n         [1.0120e+04, 2.1869e+04, 5.1342e+04],\n         [8.1460e+03, 2.3234e+04, 6.4421e+04],\n         [1.2120e+03, 1.8387e+04, 8.3959e+04],\n         [2.4600e+02, 1.7179e+04, 4.2283e+04],\n         [1.0133e+04, 1.7072e+04, 8.0345e+04],\n         [4.9960e+03, 2.0659e+04, 5.4548e+04],\n         [4.8640e+03, 1.8676e+04, 8.7977e+04],\n         [1.2912e+04, 2.0426e+04, 6.9772e+04],\n         [1.6588e+04, 1.7504e+04, 4.0960e+04],\n         [3.3480e+03, 1.7657e+04, 4.9264e+04],\n         [9.1570e+03, 2.3342e+04, 5.7805e+04],\n         [2.6910e+03, 1.8167e+04, 7.4080e+04],\n         [1.6425e+04, 1.7106e+04, 6.7761e+04],\n         [1.2240e+04, 2.1883e+04, 5.7557e+04],\n         [9.1570e+03, 3.7209e+04, 4.2610e+04],\n         [1.3354e+04, 1.8251e+04, 4.2265e+04],\n         [1.2959e+04, 2.6111e+04, 4.1579e+04],\n         [9.1570e+03, 3.1732e+04, 6.8839e+04],\n         [8.7120e+03, 2.0023e+04, 7.7806e+04],\n         [1.6859e+04, 2.1480e+04, 7.1054e+04],\n         [1.2169e+04, 1.8854e+04, 6.1399e+04],\n         [1.1644e+04, 2.5237e+04, 4.6925e+04],\n         [2.1630e+03, 1.7350e+04, 4.4568e+04],\n         [1.5089e+04, 1.9054e+04, 5.6523e+04],\n         [1.6905e+04, 1.9614e+04, 4.9904e+04],\n         [6.8220e+03, 1.7188e+04, 6.4199e+04],\n         [1.4044e+04, 3.1399e+04, 8.1689e+04],\n         [7.9170e+03, 2.3299e+04, 7.5179e+04],\n         [6.0340e+03, 1.7108e+04, 5.2939e+04],\n         [1.3859e+04, 4.0635e+04, 5.6399e+04],\n         [1.5933e+04, 1.8570e+04, 6.1397e+04],\n         [4.8260e+03, 1.8674e+04, 8.4171e+04],\n         [1.5626e+04, 1.7857e+04, 4.7907e+04],\n         [1.6117e+04, 1.9144e+04, 6.4282e+04],\n         [7.7480e+03, 1.8205e+04, 4.1288e+04],\n         [1.1496e+04, 1.8009e+04, 6.9181e+04],\n         [1.4845e+04, 1.7260e+04, 7.6415e+04],\n         [9.8600e+02, 2.2718e+04, 5.4399e+04],\n         [7.0420e+03, 1.8559e+04, 4.7654e+04],\n         [1.5776e+04, 1.8016e+04, 7.1341e+04],\n         [1.2057e+04, 2.3011e+04, 6.6096e+04],\n         [8.9900e+03, 1.8017e+04, 4.1321e+04],\n         [1.6981e+04, 2.8139e+04, 6.0138e+04],\n         [6.9060e+03, 2.2383e+04, 6.1162e+04],\n         [1.3190e+04, 1.7107e+04, 5.2121e+04],\n         [3.2720e+03, 1.7187e+04, 7.5652e+04],\n         [1.2598e+04, 1.8885e+04, 5.3772e+04],\n         [1.0801e+04, 1.7164e+04, 7.0916e+04],\n         [1.0050e+03, 1.8980e+04, 4.2699e+04],\n         [1.6764e+04, 2.7919e+04, 8.5027e+04],\n         [1.3498e+04, 2.1994e+04, 4.4798e+04],\n         [7.2000e+03, 2.1933e+04, 8.1523e+04],\n         [5.2350e+03, 2.0122e+04, 6.9574e+04],\n         [1.4002e+04, 2.4111e+04, 4.4371e+04],\n         [1.6498e+04, 2.2139e+04, 8.1620e+04],\n         [6.1970e+03, 3.6253e+04, 5.8631e+04],\n         [1.5933e+04, 1.7109e+04, 6.7471e+04],\n         [6.4370e+03, 1.7197e+04, 7.6221e+04],\n         [2.9420e+03, 1.8058e+04, 6.3262e+04],\n         [1.4365e+04, 2.3248e+04, 4.4735e+04],\n         [1.5252e+04, 2.5649e+04, 4.2541e+04],\n         [3.2990e+03, 1.9996e+04, 5.4811e+04],\n         [9.2390e+03, 1.9441e+04, 6.5079e+04],\n         [1.3464e+04, 1.8650e+04, 6.0268e+04],\n         [4.4800e+02, 2.6994e+04, 5.2781e+04],\n         [1.0622e+04, 2.1202e+04, 5.7255e+04],\n         [1.5361e+04, 1.9713e+04, 6.2790e+04],\n         [1.4912e+04, 2.0212e+04, 7.4559e+04],\n         [4.2000e+01, 2.0828e+04, 5.8054e+04],\n         [4.4560e+03, 1.7252e+04, 6.9100e+04],\n         [1.8980e+03, 2.5784e+04, 4.7788e+04],\n         [5.7060e+03, 2.3544e+04, 4.7655e+04],\n         [1.6859e+04, 1.7381e+04, 4.1629e+04],\n         [1.4520e+04, 2.4889e+04, 6.6089e+04],\n         [1.0600e+02, 1.7134e+04, 7.5588e+04],\n         [1.1570e+04, 2.3027e+04, 6.5098e+04],\n         [6.7350e+03, 1.8136e+04, 6.0120e+04],\n         [7.8540e+03, 1.8383e+04, 5.5415e+04],\n         [1.5391e+04, 2.2120e+04, 7.6138e+04],\n         [1.0820e+03, 2.9431e+04, 5.0849e+04],\n         [9.9290e+03, 1.7642e+04, 7.6875e+04],\n         [1.8980e+03, 2.2233e+04, 4.6080e+04],\n         [1.0622e+04, 2.2852e+04, 8.0550e+04],\n         [1.0671e+04, 1.7182e+04, 5.0365e+04],\n         [1.6380e+03, 2.0640e+04, 5.4854e+04],\n         [1.1972e+04, 1.8664e+04, 6.1672e+04],\n         [3.5230e+03, 1.9443e+04, 7.4002e+04],\n         [1.2543e+04, 1.7152e+04, 6.8678e+04],\n         [5.4150e+03, 2.1237e+04, 4.9211e+04],\n         [1.5362e+04, 1.7913e+04, 7.8913e+04],\n         [5.7060e+03, 2.3895e+04, 4.9344e+04],\n         [1.3910e+04, 1.7316e+04, 4.8591e+04],\n         [1.5191e+04, 1.8004e+04, 4.1095e+04],\n         [1.1834e+04, 1.8053e+04, 8.8702e+04],\n         [9.3310e+03, 1.9213e+04, 4.7848e+04],\n         [1.8980e+03, 1.8306e+04, 5.4476e+04],\n         [1.4286e+04, 1.7367e+04, 5.1450e+04],\n         [1.0484e+04, 2.0909e+04, 4.3799e+04],\n         [2.7800e+02, 1.7741e+04, 7.7898e+04],\n         [1.1496e+04, 1.9667e+04, 5.4470e+04],\n         [1.3859e+04, 4.0622e+04, 5.4382e+04],\n         [8.7800e+03, 2.0562e+04, 4.2299e+04],\n         [8.1750e+03, 1.8384e+04, 4.3206e+04],\n         [1.4520e+04, 2.4769e+04, 5.7539e+04],\n         [1.2057e+04, 1.8265e+04, 5.0374e+04],\n         [1.8980e+03, 1.7341e+04, 8.0476e+04],\n         [5.5520e+03, 1.8000e+04, 8.5023e+04],\n         [3.5340e+03, 1.9461e+04, 6.7682e+04],\n         [1.3134e+04, 1.7445e+04, 4.2137e+04],\n         [3.6800e+03, 1.7203e+04, 5.4932e+04],\n         [1.5379e+04, 1.8251e+04, 7.5755e+04],\n         [1.2455e+04, 2.4379e+04, 5.3534e+04],\n         [3.6680e+03, 1.7515e+04, 7.6063e+04],\n         [7.1450e+03, 1.9200e+04, 6.5321e+04],\n         [1.3400e+04, 2.2335e+04, 4.2296e+04],\n         [1.2057e+04, 1.7803e+04, 5.0654e+04],\n         [5.9610e+03, 1.7367e+04, 4.2215e+04],\n         [1.1496e+04, 1.7873e+04, 4.8441e+04],\n         [5.8510e+03, 2.0149e+04, 5.1667e+04],\n         [4.5000e+01, 2.1915e+04, 4.7134e+04],\n         [1.1646e+04, 1.8603e+04, 7.0794e+04],\n         [1.7480e+03, 1.7072e+04, 4.3410e+04],\n         [1.0523e+04, 1.7123e+04, 7.8228e+04],\n         [1.8980e+03, 2.8099e+04, 4.2492e+04],\n         [5.5370e+03, 1.8225e+04, 7.9391e+04],\n         [1.4646e+04, 1.8547e+04, 5.8435e+04],\n         [3.5750e+03, 1.7176e+04, 7.7513e+04],\n         [5.1760e+03, 1.8117e+04, 6.4742e+04],\n         [1.6056e+04, 1.7776e+04, 4.6270e+04],\n         [1.4964e+04, 1.7055e+04, 7.1765e+04],\n         [5.1940e+03, 1.7470e+04, 4.1331e+04],\n         [2.0020e+03, 1.7085e+04, 4.8341e+04],\n         [1.2169e+04, 1.7312e+04, 7.9280e+04],\n         [1.0263e+04, 2.0104e+04, 4.1538e+04],\n         [7.1450e+03, 1.9512e+04, 7.4206e+04],\n         [1.0833e+04, 1.7879e+04, 4.9531e+04],\n         [9.7140e+03, 2.5695e+04, 4.3709e+04],\n         [1.5570e+04, 1.7473e+04, 7.6060e+04],\n         [4.5290e+03, 2.3227e+04, 4.9841e+04],\n         [5.4700e+02, 2.2483e+04, 4.2426e+04],\n         [3.1390e+03, 1.9001e+04, 6.1568e+04],\n         [2.2620e+03, 1.8528e+04, 7.0960e+04],\n         [9.8370e+03, 1.9514e+04, 5.0922e+04],\n         [1.2112e+04, 1.7197e+04, 8.6439e+04],\n         [2.3100e+03, 1.7928e+04, 8.6564e+04],\n         [9.1570e+03, 2.7090e+04, 5.9227e+04],\n         [7.3590e+03, 1.9898e+04, 7.4133e+04],\n         [8.9480e+03, 2.0354e+04, 8.3403e+04],\n         [9.9330e+03, 3.9630e+04, 4.9324e+04],\n         [5.1160e+03, 2.0114e+04, 5.0221e+04],\n         [1.5191e+04, 1.7110e+04, 6.9470e+04],\n         [1.2430e+04, 2.3276e+04, 5.6775e+04],\n         [9.1570e+03, 2.6738e+04, 6.1815e+04],\n         [6.5100e+03, 1.8473e+04, 6.0611e+04],\n         [1.4631e+04, 1.7957e+04, 5.1926e+04],\n         [1.1106e+04, 2.6738e+04, 4.6736e+04],\n         [1.2524e+04, 2.0556e+04, 4.3397e+04],\n         [1.8770e+03, 1.8646e+04, 5.8537e+04],\n         [1.4008e+04, 1.7285e+04, 5.6498e+04],\n         [8.5240e+03, 1.9100e+04, 4.3160e+04],\n         [1.2955e+04, 1.7336e+04, 4.9267e+04],\n         [3.5230e+03, 1.8130e+04, 4.2286e+04],\n         [1.2120e+03, 1.8094e+04, 4.2514e+04],\n         [5.0930e+03, 1.7471e+04, 4.9145e+04],\n         [4.7820e+03, 1.9226e+04, 5.1403e+04],\n         [9.8990e+03, 1.9368e+04, 4.1599e+04],\n         [3.8420e+03, 3.5214e+04, 7.5024e+04],\n         [1.3563e+04, 2.3047e+04, 4.7036e+04],\n         [1.6369e+04, 2.7041e+04, 7.2400e+04],\n         [1.6588e+04, 2.5461e+04, 8.1405e+04],\n         [7.0520e+03, 2.2334e+04, 6.4971e+04],\n         [5.1500e+02, 2.8954e+04, 4.9175e+04],\n         [1.6979e+04, 1.8014e+04, 4.1468e+04],\n         [1.2120e+03, 1.7648e+04, 8.4721e+04],\n         [2.2620e+03, 1.9981e+04, 5.1768e+04],\n         [5.7060e+03, 1.8835e+04, 6.5182e+04]], dtype=torch.float64),\n tensor([ 1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n         -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n         -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n          1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n         -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,\n          1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,\n          1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,\n         -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n         -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n         -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n          1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n         -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,\n          1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n          1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,\n         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n         -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n          1., -1., -1., -1.], dtype=torch.float64)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 31
    }
   ],
   "source": [
    "a\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}